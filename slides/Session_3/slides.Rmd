---
title: "Intro to Data Analysis with R: Session 3"
subtitle: UCI Data Science Initiative
params: 
  reportdate: "x"  # to be replaced
date: "`r params$reportdate`"
#author: "Chris Galbraith"
output: slidy_presentation
smaller: yes
---

```{r, include=FALSE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## Session 3 Agenda

1. Statistical Distributions (very brief) 

2. T-Tests

3. Linear Regression

    + Fitting Models
    + Interpretation
    + Comparing Models & Variable Selection
    + Diagnostics
    + Prediction


## Statistical Distributions in R:

+ R has many built-in statistical distributions
    + e.g., binomial, poisson, normal, chi square, ...

+ Each distribution in R has four functions:
    + These functions begin with a "d", "p", "q", or "r" and are followed by the name of the distribution

+ ```d<dist>()```: evaluates the probability density/mass function at a given value
+ ```r<dist>()```: generates random numbers
+ ```p<dist>()```: returns the cumulative distribution function (CDF) for a given quantile
+ ```q<dist>()```: returns the quantile for a given probability


## Standard Normal Distribution

+ Calculate the value of the probability density function at $X = 0$
```{r echo=TRUE}  
str(dnorm) # normal pdf
dnorm(x = 0, mean = 0, sd = 1)
```


## Standard Normal Distribution
```{r echo=TRUE, fig.height = 4.5, fig.align='center'}  
x <- seq(from = -3, to = 3, by = 0.05)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l", ylab = "Density")
```

## Standard Normal Distribution

+ Generate 10 independent random numbers from a standard normal distribution
```{r echo=TRUE}  
str(rnorm) # generate random number from normal dist
rnorm(10, mean = 0, sd = 1)
```


## Standard Normal Distribution

+ Calculate the probability that $X \leq 0$
```{r echo=TRUE}  
str(pnorm) # normal CDF
pnorm(0, mean = 0, sd = 1) # Pr[X <= 0] = ?
```


## Standard Normal Distribution

+ Find the value for which the CDF = 0.975
```{r echo=TRUE}  
str(qnorm) # normal quantile func
qnorm(0.975, mean = 0, sd = 1) # PR[X <= ?] = 0.975
```


## T-Tests
T-tests can be used to draw statistical conclusions about parameters of interest in the data

+ Is the mean of this data different from zero (or another number)?

+ Are the means of two data sets different from one another?

+ Is the regression slope coefficient different from zero?

T-tests can be categorized into two groups:

1. One-sample t-test

2. Two-sample t-test


##  One-Sample T-Test (Create Data)
```{r echo=TRUE}
set.seed(123)
oneSampData <- rnorm(100, mean = 0, sd = 1)
mean(oneSampData)
sd(oneSampData)
```


##  One-Sample T-Test ($H_0: \mu = 0$)
```{r echo=TRUE}
oneSampTest.0 <- t.test(oneSampData) 
oneSampTest.0
```

```{r echo=TRUE}
names(oneSampTest.0) 
oneSampTest.0$statistic
oneSampTest.0$estimate
```  


##  One-Sample T-Test ($H_0: \mu = a$)
```{r echo=TRUE}
a <- 0.3
oneSampTest.mu <- t.test(oneSampData, mu = a)
oneSampTest.mu
```  


##  Two-Sample T-Test
Two sample t-tests are categorized into 3 groups:

  + T-Test with equal variances
  
  + T-Test with un-equal variances
  
  + Paired T-Test (one-sample t-test on differences)


##  Two-Sample T-Test (Create & Plot Data)
```{r echo = TRUE}
Samp1 <- rnorm(300, mean = 2.5, sd = 1)
Samp2 <- rnorm(500, mean = 3.0, sd = 1) # notice: not the same sample size
plot(density(Samp1), col="red", main="Densities of Samp1 and Samp2", xlab="")
abline(v = mean(Samp1), col = "red", lwd = 2, lty=2)
lines(density(Samp2), col="blue")
abline(v = mean(Samp2), col = "blue", lwd = 2, lty = 2)
legend("topright", legend = c("Samp1", "Samp2"),
       fill = c("red","blue"), bty = "n", cex = 1.3)
```


##  Two-Sample T-Test (Un-equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2)  # default assump: unequal variances
```


##  Two-Sample T-Test (Equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2, var.equal = TRUE)  # default assump: unequal variances
```


##  Two-Sample T-Test (Paired T-Test)
Let $D \equiv \{x_i - y_i : i=1, \ldots, n \}$.

Null hypothesis: $\mu_D = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2[1:300], paired = TRUE) # must be of the same sample size
```


## Linear Regression
+ Approach to model the relationship between a *response* variable $Y$ and one or more *predictor* variables $X$.

+ $Y$ is a noisy linear combination of $X$: $$Y = X \beta + \epsilon$$

    + $Y$ is a $n \times 1$ vector of response variables
    + $X$ is a $n \times p$ matrix where each row $X_i$ is the predictor variables for the $i^{th}$ observation
    + $\beta$ is the $p \times 1$ vector of regression coefficients
    + $\epsilon \sim N(0,\sigma^2 I)$ is the $n \times 1$ error vector 
    + Implies that $Y \sim N(X\beta, \sigma^2I)$

+ Linear regression equivalent to modeling the mean of $Y$. Namely, $\mu = E(Y) = X\beta$

+ Multiple ways to estimate $\beta$, but we will focus on the most basic method which is **ordinary least squares estimation**.

    + Minimize sum of squared error (SSE): $\sum_{i=1}^{n} (y_i - X_i\beta)^2$
    + Closed form solution: $\hat{\beta} = (X^T X)^{-1} X^T Y$
    + Unbiased and consistent


## Linear Regression in R
+ We use ```lm()``` to fit linear regression models, which has requires us to pass in a regression formula.

+ Don't need to create the matrix $X$ (R does this for us), just tell it which predictors to include.

+ Some notes on ```formula```:

    + Response variable goes on the left and the predictors go on the right of a ```~```
    + Predictors are separated by ```+``` symbols (e.g., ```y ~ x + z```)
    + If you want to use all variables in a data frame, simply use ```.``` (e.g., ```y ~ .```)
    + By default, the formula contains an intercept. If you want to exclude it (i.e., force the intercept to be zero), can use either of ```y ~ 0 + .``` or ```y ~ . - 1```
    + Can include polynomial terms: Say we want to have a quadratic regression of ```y``` on ```x```, we simply use ```y ~ x + I(x^2)```


## Linear Regression - Prestige Data
We will fit a linear model using the prestige data. Recall that we have the following variables

+ ```education```: Average education of occupational incumbents, years, in 1971.

+ ```income```: Average income of incumbents, dollars, in 1971.

+ ```women```: Percentage of incumbents who are women.

+ ```prestige```: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.

+ ```census```: Canadian Census occupational code.

+ ```type```: Type of occupation, a factor with levels

    + ```bc```: Blue Collar
    + ```prof```: Professional, Managerial, and Technical
    + ```wc```: White Collar


## Load Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
prestige <- read.csv(file = here::here("data", "prestige_v2.csv"), 
                     row.names=1)
str(prestige)
head(prestige)
```


## Another look at the scatterplot matrix
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
library(car)
scatterplotMatrix(prestige[,c("prestige","education","income","women")])
```


## Linear Regression - Fit the Model
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg <- lm(prestige ~ education + income + women, data = prestige)
myReg
names(myReg)
```


##  Linear Regression - Summary of Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(myReg)
```


##  Linear Regression - Summary Contents
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg.summary <- summary(myReg)
names(myReg.summary) # show different contents
names(myReg) # this is what we had previously
```


##  Linear Regression - Confidence Intervals
+ 95% confidence interval for coefficient of ```income```
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, 'income', level=0.95)
```

+ 95% confidence interval for each coefficient
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, level=0.95)
```


##  Linear Regression - Adding Variables
Recall that ```type``` of occupation has a relationship with prestige score.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.height=5*(3/4), fig.width=5}
boxplot(prestige ~ type, data = Prestige, col = "grey",
        main = "Distribution of Prestige Score by Types",
        xlab = "Occupation Types", ylab = "Prestige Score")
```

So let's add the predictor ```type``` into our regression model:
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


##  Linear Regression - Comparing Models
Suppose we want to test which of our two models is better when one model is *nested* within the other (i.e., both models contain the same terms and one has at least one additional term). 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
formula(myReg)
formula(mod)
```

We say that ```myReg``` is *nested within* ```mod```, or equivantley that ```myReg``` is the **reduced** model and ```mod``` is the **full** model.

Hypothesis Test:

+ Null: Reduced model fits as well as the full model (formally, coefficients for the omitted terms are all zero).
    
+ Alternative: Full model fits better than the reduced model (formally, at least one omitted coefficient is non-zero).


##  Linear Regression - Comparing Models, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
anova(myReg, mod)
```
So we reject the null hypothesis and conclude that at least one of the levels of type of occupation contributes information about the prestige score. 


##  Linear Regression - Relevel a Factor
Say we want the change the reference level for the covariate ```type``` so that the intercept of the model includes is for professionals. We simply relevel the variable ```type``` and re-fit the model. 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
levels(prestige$type)
prestige$type <- relevel(prestige$type, "prof")
levels(prestige$type)
```

**Note: This does not change the results of the model (e.g., predictions, F-statistics) but does change the interpretation and p-values of the type coefficients.**

##  Linear Regression - Relevel a Factor, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


## Linear Regression - More on Variable Selection
+ Automated algorithms for automatic variable selection exist, but should be used with caution. 

+ Better to start with an *a priori* set of variables you need to include and then compare models, especially when goal is assocation instead of prediction.

+ **Stepwise regression**: start with a model and add (or remove) predictors until some criteria is met

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8}
null <- lm(prestige ~ 1, data=prestige)  # most basic model, intercept only
full <- lm(prestige ~ education + income + women + type, data=prestige)  # saturated model, all predictors
step(null, scope=list(lower=null, upper=full), direction='forward')
step(full, scope=list(lower=null, upper=full), direction='backward')
step(null, scope=list(lower=null, upper=full), direction='both')
```


## Linear Regression - More on Variable Selection
+ **All subsets regression**: test all possible subsets of the predictors 

    + $p$ predictors $\Rightarrow 2^p$ combinations
    + Pick the best model based on some well-defined criteria
    + Pro: Exhaustive
    + Con: Computationally expensive

+ **Best subsets regression**: similar to all subsets, but only look at the best model of each size

+ Multiple packages can do this, but we will use ```leaps```

```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=5, fig.width=8}
if (!require("leaps")) install.packages("leaps")  # install leaps if it isn't already
library(leaps)
bestSubsets <- regsubsets(prestige ~ education + income + women + type, data=prestige, nbest=1)
summary(bestSubsets)
par(mfrow=c(1,2))
plot(bestSubsets, scale="adjr2"); plot(bestSubsets, scale="Cp")
```

Note: The plot on the right is Mallow's $C_p$, which is a statistic used to compare models that accounts for the number of predictors in the model (we want it to be roughly equal to the number of predictors included in the model). 


##  Linear Regression - Diagnostics
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8}
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))
plot(myReg)
```


##  Linear Regression - Prediction
Predict the output for a new input
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
newData = data.frame(education=13.2, income=12000, women=12)
predict(myReg, newData, interval="predict")
```


## End of Session 3
Next up:

1. [Exercise 3](https://ucidatascienceinitiative.github.io//IDA-with-R/Exercises/IDA-with-R_Exercise_3.html)
2. Break

**Return at 3:30 to discuss solutions to Exercise 3!**
