---
title: "Intro to Data Analysis with R: Session 3"
subtitle: UCI Data Science Initiative
date: "October 20, 2017"
#author: "Chris Galbraith"
output: slidy_presentation
smaller: yes
---

```{r, include=FALSE, echo=FALSE, warning=FALSE, error=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(car)
```


## Session 3 Agenda

1. Statistical Distributions (very brief) 

2. T-Tests

3. Linear Regression

    + Fitting Models
    + Interpretation
    + Diagnostics
    + Prediction


## Statistical Distributions in R:

+ R has many built-in statistical distributions
    + e.g., binomial, poisson, normal, chi square, ...

+ Each distribution in R has four functions:
    + These functions begin with a "d", "p", "q", or "r" and are followed by the name of the distribution

+ ```d<dist>()```: evaluates the probability density/mass function at a given value
+ ```r<dist>()```: generates random numbers
+ ```p<dist>()```: returns the cumulative distribution function (CDF) for a given quantile
+ ```q<dist>()```: returns the quantile for a given probability


## Standard Normal Distribution

+ Calculate the value of the probability density function at $X = 0$
```{r echo=TRUE}  
str(dnorm) # normal pdf
dnorm(x = 0, mean = 0, sd = 1)
```


## Standard Normal Distribution
```{r echo=TRUE, fig.height = 4.5, fig.align='center'}  
x <- seq(from = -3, to = 3, by = 0.05)
y <- dnorm(x, mean = 0, sd = 1)
plot(x, y, type = "l")
```

## Standard Normal Distribution

+ Generate 10 independent random numbers from a standard normal distribution
```{r echo=TRUE}  
str(rnorm) # generate random number from normal dist
rnorm(10, mean = 0, sd = 1)
```


## Standard Normal Distribution

+ Calculate the probability that $X \leq 0$
```{r echo=TRUE}  
str(pnorm) # normal CDF
pnorm(0, mean = 0, sd = 1) # Pr[X <= 0] = ?
```


## Standard Normal Distribution

+ Find the value for which the CDF = 0.975
```{r echo=TRUE}  
str(qnorm) # normal quantile func
qnorm(0.975, mean = 0, sd = 1) # PR[X <= ?] = 0.975
```


## T-Tests
T-tests can be used to draw statistical conclusions about parameters of interest in the data

+ Is the mean of this data different from zero (or another number)?

+ Are the means of two data sets different from one another?

+ Is the regression slope coefficient different from zero?

T-tests can be categorized into two groups:

1. One-sample t-test

2. Two-sample t-test


##  One-Sample T-Test (Create Data)
```{r echo=TRUE}
set.seed(123)
oneSampData <- rnorm(100, mean = 0, sd = 1)

mean(oneSampData)
sd(oneSampData)
```


##  One-Sample T-Test ($H_0: \mu = 0$)
```{r echo=TRUE}
oneSampTest.0 <- t.test(oneSampData) 
oneSampTest.0
```

```{r echo=TRUE}
names(oneSampTest.0) 
oneSampTest.0$statistic
oneSampTest.0$estimate
```  


##  One-Sample T-Test ($H_0: \mu = a$)
```{r echo=TRUE}
a <- 0.3
oneSampTest.mu <- t.test(oneSampData, mu = a)
oneSampTest.mu
```  


##  Two-Sample T-Test
Two sample t-tests are categorized into 3 groups:

  + T-Test with equal variances
  
  + T-Test with un-equal variances
  
  + Paired T-Test (one-sample t-test on differences)


##  Two-Sample T-Test (Create & Plot Data)
```{r echo = TRUE}
Samp1 <- rnorm(300, mean = 2.5, sd = 1)
Samp2 <- rnorm(500, mean = 3.0, sd = 1) # notice: not the same sample size
plot(density(Samp1), col="red", main="Densities of Samp1 and Samp2", xlab="")
abline(v = mean(Samp1), col = "red", lwd = 2, lty=2)
lines(density(Samp2), col="blue")
abline(v = mean(Samp2), col = "blue", lwd = 2, lty = 2)
legend("topright", legend = c("Samp1", "Samp2"),
       fill = c("red","blue"), bty = "n", cex = 1.3)
```


##  Two-Sample T-Test (Un-equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2)  # default assump: unequal variances
```


##  Two-Sample T-Test (Equal Variances)
Null hypothesis: $\mu_1 = \mu_2 \Leftrightarrow \mu_1 - \mu_2 = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2, var.equal = TRUE)  # default assump: unequal variances
```


##  Two-Sample T-Test (Paired T-Test)
Let $D \equiv \{x_i - y_i : i=1, \ldots, n \}$, then

Null hypothesis: $\mu_D = 0$
```{r echo = TRUE}
t.test(Samp1, Samp2[1:300], paired = TRUE) # must be of the same sample size
```


## Linear Regression
Add notes on basic theory with references!


## Linear Regression in R
We use ```lm()``` to fit linear regression models, which has requires us to pass in a regression formula.

Some notes on ```formula```:

+ Response variable goes on the left and the predictors go on the right of a ```~```

+ Predictors are separated by ```+``` symbols (e.g., ```y ~ x + z```)

+ If you want to use all variables in a data frame, simply use ```.``` (e.g., ```y ~ .```)

+ By default, the formula contains an intercept. If you want to exclude it (i.e., force the intercept to be zero), can use either of ```y ~ 0 + .``` or ```y ~ . - 1```

+ Can include polynomial terms: Say we want to have a quadratic regression of ```y``` on ```x```, we simply use ```y ~ x + I(x^2)```


## Linear Regression - Prestige Data
We will fit a linear model using the Prestige data. Recall that we have the following variables

+ education: Average education of occupational incumbents, years, in 1971.

+ income: Average income of incumbents, dollars, in 1971.

+ women: Percentage of incumbents who are women.

+ prestige: Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s.

+ census: Canadian Census occupational code.

+ type: Type of occupation, a factor with levels 

    + bc: Blue Collar
    + prof: Professional, Managerial, and Technical
    + wc: White Collar


## Load Data
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
prestige <- read.csv(file="/Users/CMG/dev/IDA-with-R/data/prestige_v2.csv", 
                     row.names=1)  # path will vary
str(prestige)
head(prestige)
```


## Another look at the scatterplot matrix
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
scatterplotMatrix(prestige[,c("prestige","education","income","women")])
```


## Linear Regression - Fit the Model
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg <- lm(prestige ~ education + income + women, data = prestige)
myReg
names(myReg)
```


##  Linear Regression - Summary of Fit
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
summary(myReg)
```


##  Linear Regression - Summary Contents
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
myReg.summary <- summary(myReg)
names(myReg.summary) # show different contents
names(myReg) # this is what we had previously
```


##  Linear Regression - Confidence Interval
+ 95% confidence interval for coefficient of 'income'
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, 'income', level=0.95)
```

+ 95% confidence interval for each coefficient
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
confint(myReg, level=0.95)
```


##  Linear Regression - Adding Variables
Recall that type of occupation has a relationship with prestige score.
```{r echo=FALSE, error=FALSE, message=FALSE, warning=FALSE, fig.height=5*(3/4), fig.width=5}
boxplot(prestige ~ type, data = Prestige, col = "grey",
        main = "Distribution of Prestige Score by Types",
        xlab = "Occupation Types", ylab = "Prestige Score")
```

So let's add the predictor *type* into our regression model:
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


##  Linear Regression - Comparing Models
Suppose we want to test which of our two models is better when one model is *nested* within the other (i.e., both models contain the same terms and one has at least one additional term). 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
formula(myReg)
formula(mod)
```

We say that ```myReg``` is *nested within* ```mod```, or equivantley that ```myReg``` is the **reduced** model and ```mod``` is the **full** model.

Hypothesis Test:

+ Null: Reduced model fits as well as the full model (formally, coefficients for the omitted terms are all zero).
    
+ Alternative: Full model fits better than the reduced model (formally, at least one omitted coefficient is non-zero).


##  Linear Regression - Comparing Models, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
anova(myReg, mod)
```
So we reject the null hypothesis and conclude that at least one of the levels of *type* contributes information about the prestige score. 


##  Linear Regression - Relevel a Factor
Say we want the change the reference level for the covariate *type* so that the intercept of the model includes is for professionals. We simply relevel the variable *type* and re-fit the model. 
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
levels(prestige$type)
prestige$type <- relevel(prestige$type, "prof")
levels(prestige$type)
```

**Note: This does not change the results of the model (e.g., predictions, F-statistics) but does change the interpretation and p-values of the type coefficients.**

##  Linear Regression - Relevel a Factor, contd.
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
mod <- update(myReg, ~ . + type)
summary(mod)
```


##  Linear Regression - Diagnostics
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8}
par(mfrow = c(2, 2), oma = c(0, 0, 2, 0))
plot(myReg)
```


##  Linear Regression - Prediction
Predict the output for a new input
```{r echo=TRUE, error=FALSE, message=FALSE, warning=FALSE}
newData = data.frame(education=13.2, income=12000, women=12)
predict(myReg, newData, interval="predict")
```


## Linear Regression - Variable Selection
Add notes on forward, backward, stepwise, and all subsets regression.


## End of Session 3
Next up:

1. Exercise 3
2. Break

Return at XXXX to discuss answers to Exercise 3!



